import spacy
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from typing import Dict, List, Optional, Tuple
from utils import preprocess_text, load_hadith_data

class TextAnalyzer:
    def __init__(self):
        """Initialize NLP models and pipelines."""
        # Load spaCy Arabic model for NER
        try:
            self.nlp = spacy.load("xx_ent_wiki_sm")
        except OSError:
            print("âš ï¸ spaCy model not found. Please install with: python -m spacy download xx_ent_wiki_sm")
            self.nlp = None
        
        # Load transformers pipelines
        try:
            print("Chargement du modÃ¨le de rÃ©sumÃ©...")
            self.summarizer = pipeline(
                "summarization",
                model="csebuetnlp/mT5_multilingual_XLSum",
                tokenizer="csebuetnlp/mT5_multilingual_XLSum"
            )
            self.use_summarizer = True
        except Exception as e:
            print(f"âš ï¸ Erreur lors du chargement du modÃ¨le de rÃ©sumÃ©: {e}")
            print("âš ï¸ Utilisation d'une mÃ©thode de rÃ©sumÃ© alternative")
            self.summarizer = None
            self.use_summarizer = False
            
        try:
            print("Chargement du modÃ¨le d'analyse de sentiment...")
            self.sentiment_analyzer = pipeline(
                "text-classification",
                model="CAMeL-Lab/bert-base-arabic-camelbert-ca",
                tokenizer="CAMeL-Lab/bert-base-arabic-camelbert-ca"
            )
        except Exception as e:
            print(f"âš ï¸ Erreur lors du chargement du modÃ¨le de sentiment: {e}")
            self.sentiment_analyzer = None
        
        # Load hadith data for comparison
        try:
            self.hadith_df = load_hadith_data()
            # Conserver les mÃ©tadonnÃ©es avec les textes pour les retrouver plus tard
            self.hadith_texts = self.hadith_df['text'].tolist()
            self.hadith_metadata = self.hadith_df.to_dict('records')
        except Exception as e:
            print(f"âŒ Failed to load hadith data: {e}")
            self.hadith_df = pd.DataFrame()
            self.hadith_texts = []
            self.hadith_metadata = []
        
        # Load embedding model for similarity
        try:
            print("Chargement du modÃ¨le d'embedding...")
            self.embedding_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            self.tokenizer = AutoTokenizer.from_pretrained(self.embedding_model)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.embedding_model)
        except Exception as e:
            print(f"âš ï¸ Erreur lors du chargement du modÃ¨le d'embedding: {e}")
            self.tokenizer = None
            self.model = None

    def get_text_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for a given text."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        outputs = self.model(**inputs)
        return outputs.logits.detach().numpy().mean(axis=0)

    def extract_key_sentences(self, text, num_sentences=3):
        """Extraire les phrases les plus importantes du texte comme mÃ©thode alternative de rÃ©sumÃ©."""
        # Diviser le texte en phrases
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if not sentences:
            return text
            
        if len(sentences) <= num_sentences:
            return '. '.join(sentences) + '.'
            
        # Compter les mots dans chaque phrase
        word_counts = [len(s.split()) for s in sentences]
        
        # Ã‰liminer les phrases trop courtes (probablement pas informatives)
        valid_sentences = [(i, s) for i, s in enumerate(sentences) if len(s.split()) >= 5]
        
        if not valid_sentences:
            # Si toutes les phrases sont trop courtes, prendre les premiÃ¨res
            return '. '.join(sentences[:num_sentences]) + '.'
            
        # Extraire les mots-clÃ©s du texte complet
        keywords = self._extract_keywords(text)
        
        # Calculer les scores des phrases en fonction des mots-clÃ©s prÃ©sents
        sentence_scores = []
        for idx, sentence in valid_sentences:
            score = sum(1 for keyword in keywords if keyword in sentence.split())
            # Bonus pour les premiÃ¨res phrases (souvent plus importantes)
            if idx < 2:
                score += 2
            sentence_scores.append((idx, sentence, score))
        
        # Trier par score dÃ©croissant
        sentence_scores.sort(key=lambda x: x[2], reverse=True)
        
        # Prendre les meilleures phrases et les rÃ©organiser selon leur position originale
        best_sentences = [(idx, sent) for idx, sent, _ in sentence_scores[:num_sentences]]
        best_sentences.sort(key=lambda x: x[0])  # Trier par position originale
        
        # Reconstruire le rÃ©sumÃ©
        summary = '. '.join([sent for _, sent in best_sentences]) + '.'
        return summary

    def summarize_text(self, text: str, max_length: int = 100, min_length: int = 10) -> str:
        """Summarize the input text."""
        try:
            # VÃ©rifier si le texte est assez long pour Ãªtre rÃ©sumÃ©
            processed_text = preprocess_text(text)
            
            # Si le texte est trop court, retourner le texte original au lieu de le rÃ©sumer
            if len(processed_text.split()) < 30:  # Si moins de 30 mots
                return processed_text
            
            # Si le modÃ¨le de rÃ©sumÃ© est disponible et activÃ©, l'utiliser
            if self.use_summarizer and self.summarizer:
                # Ajuster les paramÃ¨tres pour les textes arabes
                summary = self.summarizer(
                    processed_text,
                    max_length=max_length,
                    min_length=min_length,
                    do_sample=False,
                    num_beams=4,  # AmÃ©liore la qualitÃ© du rÃ©sumÃ©
                    early_stopping=True
                )[0]['summary_text']
                
                # VÃ©rifier que le rÃ©sumÃ© n'est pas le message d'erreur connu
                if "Ù‡Ø°Ù‡ Ù„ÙŠØ³Øª ØµÙˆØ±Ø© Ù„Ø±Ø¬Ù„" in summary or len(summary.split()) < 5:
                    return self.extract_key_sentences(processed_text)
                    
                return summary
            else:
                # Utiliser la mÃ©thode alternative basÃ©e sur l'extraction de phrases clÃ©s
                return self.extract_key_sentences(processed_text)
                
        except Exception as e:
            # En cas d'erreur, extraire les phrases importantes comme rÃ©sumÃ©
            try:
                return self.extract_key_sentences(processed_text)
            except:
                # Si tout Ã©choue, retourner les premiers mots du texte
                short_text = processed_text[:200] + "..." if len(processed_text) > 200 else processed_text
                return short_text

    def identify_context(self, text: str) -> Dict[str, str]:
        """Identify historical, cultural, or religious context."""
        sentiment = self.sentiment_analyzer(text)[0]
        keywords = self._extract_keywords(text)
        
        context = {
            "Ø§Ù„Ù…Ø´Ø§Ø¹Ø±": sentiment['label'],
            "Ø§Ù„Ø«Ù‚Ø©": f"{sentiment['score']:.2f}",
            "Ø§Ù„ÙƒÙ„Ù…Ø§Øª_Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©": ", ".join(keywords),
            "Ø§Ù„Ø³ÙŠØ§Ù‚_Ø§Ù„Ù…Ø³ØªÙ†ØªØ¬": self._infer_context_from_keywords(keywords)
        }
        return context

    def _extract_keywords(self, text: str) -> List[str]:
        """Extract key terms from text (simplified)."""
        tokens = preprocess_text(text).split()
        from collections import Counter
        word_freq = Counter(tokens)
        return [word for word, count in word_freq.most_common(5)]

    def _infer_context_from_keywords(self, keywords: List[str]) -> str:
        """Infer context based on keywords."""
        religious_terms = ['Ø§Ù„Ù„Ù‡', 'Ø±Ø³ÙˆÙ„', 'Ø­Ø¯ÙŠØ«', 'Ù‚Ø±Ø¢Ù†', 'ØµÙ„Ø§Ø©']
        historical_terms = ['Ø³Ù†Ø©', 'Ù‡Ø¬Ø±ÙŠØ©', 'Ø®Ù„ÙŠÙØ©', 'Ù…Ø¹Ø±ÙƒØ©']
        
        if any(term in keywords for term in religious_terms):
            return "Ø¯ÙŠÙ†ÙŠ (Ø¥Ø³Ù„Ø§Ù…ÙŠ)"
        elif any(term in keywords for term in historical_terms):
            return "ØªØ§Ø±ÙŠØ®ÙŠ (Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ)"
        else:
            return "Ø¹Ø§Ù…"

    def detect_entities(self, text: str) -> Dict[str, List[str]]:
        """Detect named entities using spaCy."""
        if not self.nlp:
            return {"Ø®Ø·Ø£": "Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ spaCy"}
        
        doc = self.nlp(text)
        entities = {"Ø§Ù„Ø£Ø´Ø®Ø§Øµ": [], "Ø§Ù„Ø£Ù…Ø§ÙƒÙ†": [], "Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®": [], "Ø§Ù„Ù…Ù†Ø¸Ù…Ø§Øª": []}
        
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                entities["Ø§Ù„Ø£Ø´Ø®Ø§Øµ"].append(ent.text)
            elif ent.label_ == "GPE":
                entities["Ø§Ù„Ø£Ù…Ø§ÙƒÙ†"].append(ent.text)
            elif ent.label_ == "DATE":
                entities["Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®"].append(ent.text)
            elif ent.label_ == "ORG":
                entities["Ø§Ù„Ù…Ù†Ø¸Ù…Ø§Øª"].append(ent.text)
        
        return entities

    def evaluate_reliability(self, text: str) -> Dict[str, str]:
        """Evaluate text reliability by checking contradictions and sentiment."""
        sentences = text.split('.')
        contradictions = []
        sentiment_scores = []
        
        for i, sent in enumerate(sentences):
            sent = sent.strip()
            if not sent:
                continue
            sentiment = self.sentiment_analyzer(sent)[0]
            sentiment_scores.append((sent, sentiment['label'], sentiment['score']))
        
        # Check for conflicting sentiments
        for i in range(len(sentiment_scores) - 1):
            sent1, label1, score1 = sentiment_scores[i]
            sent2, label2, score2 = sentiment_scores[i + 1]
            if label1 != label2 and score1 > 0.7 and score2 > 0.7:
                contradictions.append(f"ØªÙ†Ø§Ù‚Ø¶ Ø¨ÙŠÙ†: '{sent1}' ({label1}) Ùˆ '{sent2}' ({label2})")
        
        reliability = {
            "Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª": contradictions if contradictions else ["Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªÙ†Ø§Ù‚Ø¶Ø§Øª"],
            "Ø§ØªØ³Ø§Ù‚_Ø§Ù„ØªÙ‚ÙŠÙŠÙ…": "Ù…ØªØ³Ù‚" if not contradictions else "ØºÙŠØ± Ù…ØªØ³Ù‚",
            "Ù…Ù„Ø§Ø­Ø¸Ø©_Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©": "Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠ Ù„Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ù†Øµ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…ÙˆØ«ÙˆÙ‚ÙŠØªÙ‡ØŒ Ù„ÙƒÙ† ÙˆØ¬ÙˆØ¯ ØªÙ†Ø§Ù‚Ø¶Ø§Øª ÙŠÙ‚Ù„Ù„ Ù…Ù† Ù…ØµØ¯Ø§Ù‚ÙŠØªÙ‡."
        }
        return reliability

    def compare_with_hadith(self, text: str, threshold: float = 0.8) -> Dict[str, any]:
        """
        Compare text with hadith database for similarity.
        Returns similarity metrics and metadata of the most similar hadiths.
        """
        if not self.hadith_texts:
            return {"Ø®Ø·Ø£": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø­Ø§Ø¯ÙŠØ« Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"}
        
        text_embedding = self.get_text_embedding(text)
        similarities = []
        
        # Limiter Ã  100 hadiths pour Ã©viter les calculs trop longs
        max_hadiths = min(100, len(self.hadith_texts))
        
        for i in range(max_hadiths):
            hadith_embedding = self.get_text_embedding(self.hadith_texts[i])
            sim = cosine_similarity([text_embedding], [hadith_embedding])[0][0]
            similarities.append((sim, i))
        
        # Trier par similaritÃ© dÃ©croissante
        similarities.sort(reverse=True)
        
        # RÃ©cupÃ©rer les 3 hadiths les plus similaires
        top_matches = similarities[:3]
        similar_hadiths = []
        
        for sim, idx in top_matches:
            if idx < len(self.hadith_metadata):
                metadata = self.hadith_metadata[idx]
                hadith_info = {
                    "similaritÃ©": f"{sim:.2f}",
                    "texte": self.hadith_texts[idx][:100] + "..." if len(self.hadith_texts[idx]) > 100 else self.hadith_texts[idx]
                }
                
                # Ajouter les mÃ©tadonnÃ©es disponibles
                for key in ['source', 'hadith_no', 'hadith_id', 'chapter']:
                    if key in metadata:
                        hadith_info[key] = metadata[key]
                
                similar_hadiths.append(hadith_info)
        
        max_similarity = top_matches[0][0] if top_matches else 0
        
        result = {
            "Ø£Ù‚ØµÙ‰_Ø§Ù„ØªØ´Ø§Ø¨Ù‡": f"{max_similarity:.2f}",
            "Ù…Ø´Ø§Ø¨Ù‡": "Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©" if max_similarity > threshold else "ØºÙŠØ± Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©",
            "Ù…Ù„Ø§Ø­Ø¸Ø©_Ø§Ù„Ø£ØµØ§Ù„Ø©": "Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ø§Ù„ÙŠ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø£ØµØ§Ù„Ø©ØŒ Ù„ÙƒÙ† ÙŠØªØ·Ù„Ø¨ Ù…Ø²ÙŠØ¯Ù‹Ø§ Ù…Ù† Ø§Ù„ØªØ­Ù‚Ù‚.",
            "Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ«_Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©": similar_hadiths
        }
        
        return result
    def scientific_analysis(self, text: str) -> Dict[str, str]:
        """Fournir une analyse philologique, codicologique et historique de base."""
        keywords = self._extract_keywords(text)
        
        analysis = {
            "palÃ©ographie": "âŒ Non applicable sans image manuscrite",
            "diplomatique": "âœ… Formules classiques dÃ©tectÃ©es" if any(k in keywords for k in ['Ù‚Ø§Ù„', 'Ø³Ù…Ø¹Øª', 'Ø­Ø¯Ø«Ù†Ø§']) else "âŒ Aucune formule diplomatique claire",
            "codicologie": "âŒ Non applicable sans mÃ©tadonnÃ©es physiques",
            "philologie": f"ğŸ“˜ Mots frÃ©quents : {', '.join(keywords)}",
            "critique_textuelle": "ğŸ” Analyse limitÃ©e (pas de variantes disponibles)",
            "chronologie": self._infer_chronology(keywords),
            "historiographie": self._infer_historiography(keywords)
        }
        return analysis

    def _infer_chronology(self, keywords: List[str]) -> str:
        if any(k in keywords for k in ['Ù‡Ø¬Ø±ÙŠØ©', 'Ø³Ù†Ø©', 'Ù‚Ø±Ù†']):
            return "ğŸ“… Ã‰lÃ©ments de datation repÃ©rÃ©s"
        return "âŒ Aucune rÃ©fÃ©rence temporelle"

    def _infer_historiography(self, keywords: List[str]) -> str:
        if any(k in keywords for k in ['Ø±ÙˆØ§Ø©', 'Ø¥Ø³Ù†Ø§Ø¯', 'Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ', 'Ù…Ø³Ù„Ù…', 'Ø§Ù„Ø·Ø¨Ø±ÙŠ']):
            return "ğŸ§¾ Mention dâ€™autoritÃ©s ou chaÃ®nes de transmission"
        return "âŒ Pas de trace historiographique explicite"

def analyze_text(text: str) -> Dict[str, any]:
    """Main function to analyze a given text."""
    analyzer = TextAnalyzer()
    
    results = {
        "Ø§Ù„Ù…Ù„Ø®Øµ": analyzer.summarize_text(text),
        "Ø§Ù„Ø³ÙŠØ§Ù‚": analyzer.identify_context(text),
        "Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª": analyzer.detect_entities(text),
        "Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©": analyzer.evaluate_reliability(text),
         "Ù…Ù‚Ø§Ø±Ù†Ø©_Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ«": analyzer.compare_with_hadith(text),
                                                                                       "analyse_scientifique": analyzer.scientific_analysis(text)
    }
    
    return results

if __name__ == "__main__":
    # Example usage
    sample_text = "Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…: Ù…Ù† ÙƒØ°Ø¨ Ø¹Ù„ÙŠ Ù…ØªØ¹Ù…Ø¯Ø§ ÙÙ„ÙŠØªØ¨ÙˆØ£ Ù…Ù‚Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ù†Ø§Ø±. Ø±ÙˆØ§Ù‡ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ."
    results = analyze_text(sample_text)
    
    print("=== Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ===")
    print(f"Ø§Ù„Ù…Ù„Ø®Øµ: {results['Ø§Ù„Ù…Ù„Ø®Øµ']}")
    print(f"Ø§Ù„Ø³ÙŠØ§Ù‚: {results['Ø§Ù„Ø³ÙŠØ§Ù‚']}")
    print(f"Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª: {results['Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª']}")
    print(f"Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©: {results['Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©']}")
    print(f"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ«: {results['Ù…Ù‚Ø§Ø±Ù†Ø©_Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ«']}")
